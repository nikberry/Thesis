\chapter{Systematic Uncertainties} \label{chap-SystematicUncertainties}

Upon studying such a decay, large statistical uncertainties, comparable to the systematic uncertainties on the measurement, arise due to the small cross-section of the $t\bar{t}+\gamma$ process and small branching fraction of the decay channel. In order to perform a scientifically solid measurement we must take into account and fully understand all systematic uncertainties associated with the analysis. To begin with, we can categorise the errors into two broad categories:

\begin{description}
	\item[Flat Rate Uncertainties] - These uncertainties manifest in the form of detector performance factors, event reconstruction algorithms, and other such aspects as theoretical cross-sections which affect the overall rate of a particular process. Each uncertainty is almost universal in that it affects nearly all analyses within the collaboration, and are thus studied within their own dedicated performance group. A more detailed description can be found in \ref{sec-FlateRateUncertainties}  
	\item[Scale-factor Uncertainties] - In analyses there are often scale factors applied to scale Monte Carlo to data in order to correct for inconsistencies between the two. These can arise due to such aspects as the theoretical input parameters of the Monte Carlo generators, which are used to model signal and background processes, not taking the true shape of the data. These types of scale factors affect all distribution shapes in an analysis and therefore must be accounted, and thus an uncertainty on the scale factor is applied by varying the value up and down by one standard deviation, $\pm \sigma$, and measuring the impact that this variation has on the final result. An in-depth description of each of these types of systematic uncertainties is given in \ref{sec-ShapeUncertainties}.
\end{description}

Once computed, the systematic uncertainties are introduced as nuisance parameters within the fitting process. The final uncertainty to be considered in the fit is the statistical uncertainty that dominates this particular decay mode. This is discussed in greater detail in section \ref{chap-Results}.

% Blah blah \cite{lumiScans} \cite{PCC}

\section{Flat Rate Uncertainties} \label{sec-FlateRateUncertainties}

\subsection{Luminosity} \label{subsec-Luminosity}

The CMS collaboration measures instantaneous and integrated luminosity in two ways; one way is by means of a coincidence trigger in the forward hadron calorimeter sub-detector, and also by counting the number of clusters measured by the pixel detectors. The former method was used at the beginning of runs in the LHC, but then ran into difficulties when the number of PU increased and shifts in calibration. This lead to the development and implementation of the pixel-based calculation in 2011 - the \emph{Pixel Cluster Counting} (PCC) method \cite{cmslumiwinter2012}.

The PCC method evaluates the number of pixel clusters that occur on average for a zero-bias event (an event triggered by the requirement of only two bunches crossing at the CMS IP). It assumes that there is a small probability that each pixel within the silicon inner detector is part of more than one track per bunch crossing, and thus it is assumed that the number of pixel clusters scales linearly with the number of interactions in any given bunch crossing. This gives an excellent measure of the luminosity within the detector. Measured rates are calibrated by the method of a Van de Meer scan \cite{White:1357865}. The total calculated integrated luminosity for the entire 2012 dataset was measured to be $23.27$ fb$^{-1}$.

Although the total integrated luminosity is measured to be the value described above, the true value that we measure is less due to a number of technical reasons. Quite often a sub-detector may encounter problems at the start of the run and may require rebooting or re-calibration, thus a period of ``dead time" is induced such that data is unable to be used for physics analysis, and therefore given the title of `bad' data. The remaining measured luminosity entitled `good' is provided to analysts by the Run Coordination team, and is measured to be $19.7$ fb$^{-1}$ for the CMS experiment with the full 2012 dataset at, where a flat rate associated uncertainty of 2.6\% is assigned \cite{CMS-PAS-LUM-13-001}. Each simulated sample used in analysis is scaled to the luminosity of the dataset used, and thus the associated uncertainty affects the normalisation of every physics process.

\subsection{Lepton Efficiencies} \label{subsec-LeptonEfficiencies}

We calculate lepton efficiencies and associated uncertainties in order to correct for the number of leptons observed in data and those in simulation. In order to calculate lepton efficiencies, the tag-and-probe method is used \cite{tagandprobe}. The method analyses events from a Drell-Yan $Z \to l^+l^-$ sample as it contains a large number of unbiased lepton-pair events with a high purity. Using this sample, the tag-and-probe technique selects lepton-pair events such that one of the leptons is defined as the ``tag" lepton, which is selected with under much tighter requirements, and the second ``probe" lepton is selected under much looser constraints relative to the ``tag".  The ``tag" lepton candidate satisfies the trigger criteria, tight identification, and isolation requirements. The ``probe" lepton candidate is required to pass specific criteria depending on the efficiency under study. We thus create two subsets of the data, such that one contains events that pass the probe selection, and one collection that contains events that failed probe selection. We then take the efficiency of the selection to be the fraction of events that pass the probe selection criteria, defined as 

\begin{equation}
\epsilon_{all} = \epsilon_{reco.}\epsilon_{tight}\epsilon_{trig.}
\end{equation}

The tag-and-probe method is applied to electrons and muons separately, where it is applied to electrons in the barrel and endcap regions individually. The purity of the dilepton sample for the tag-and-probe is held by requiring the invariant mass of the lepton pair to fall within the mass window of the Z boson, $70 < M_{ll} < 130$ $\GeVcc$. We then divide the total lepton efficiency into three sub-divisions: the trigger efficiency of identifying a lepton candidate, the efficiency for the reconstruction algorithms to reconstruct leptons, and the efficiency for the identification and isolation selection requirements to correctly select leptons. 

In order to measure the trigger efficiencies for both electrons and muons, selected probes are require to pass normal kinematic cuts such that it must pass the HLT to be considered. We find that the trigger efficiency for muons is greater than 99\% and for electrons is greater than 95\%, with an associated uncertainty of the order of 4\% and varies depending on the used trigger. The reconstruction efficiency, $\epsilon_{reco.}$, is defined as the efficiency that an ECAL supercluster seeds an ECAL-driven electron candidate that passes the probe selection requirement, and is relative to ECAL clusters within the ECAL acceptance. We define the probe to have a reconstructed energy greater than 10 GeV, such that the supercluster lies within range of the tracker system. At this energy, we find the reconstruction efficiency to be greater than 85\%, and more than 99\% with an energy threshold of 20 GeV \cite{Khachatryan:2015hwa}.

For the case of muons, an initial ``preselection" of Z events for the tag-and-probe method is obtained by selecting two oppositely charged tracks measured in the central tracker that each have a $p_T > 25$ GeV, $|\eta| < 2.1$, and that when combined lie within the mass window of $60 < m_{\mu^+\mu^-} < 120$ GeV. When defining a ``tag" muon we require that it is matched to a preselected track, is a global and tracker muon, passes the selection described in Section \ref{chap-EventSelection}, and corresponds to a HLT muon. All other preselected tracks are considered as probes and are used in order to measure the efficiency. An efficiency of approximately 95-99\% is observed in data for all muon systems \cite{1748-0221-8-11-P11002}. 

The tag-and-probe method is applied to both data and simulated samples, and thus we compute the efficiency for MC simulation ($\epsilon_{sim.}$) and data ($\epsilon_{data}$).  We then compute the ratio of efficiencies along with the associated statistical and systematic uncertainties, given as:

\begin{equation}
\rho = \frac{\epsilon_{data}}{\epsilon_{sim.}}
\end{equation}

where the efficiencies and the ratios of the efficiencies are estimated in bins of E$_T$ and $\eta$ of the electron. The efficiencies and associated statistical and systematic uncertainties are derived by the EGamma and Muon Physics Object Groups (POGs) for electrons and muons, respectively.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ake sure correct and isolation/ID effs different to trig and reco.

\section{Shape Uncertainties} \label{sec-ShapeUncertainties}

\subsection{Parton Distribution Function} \label{subsec-PDFUncertainties}

Parton Distribution Functions (PDFs), denoted as $f_i(x, Q^2)$, give the probability of finding a parton of flavour $i$ (quark or gluon) carrying momentum fraction $x$ of the proton momentum where Q is the resolution scale of the hard interaction. Cross-sections are calculated by convoluting parton level cross-sections with PDFs. Due to the non-perturbative nature of partons and thus not being observed as free particles, we cannot fully obtain PDFs by perturbative QCD alone. The shapes of PDFs are determined from global fits to data from experimental observables in various processes, such as deep inelastic scattering (DIS), Drell-Yan, and jet data using the DGLAP evolution equation \cite{Vogt:2004mw}.

Event generators assign momentum fraction and energy to a parton based on PDFs which are calculated by taking data from various experiments, where each experiment has an associated uncertainty. The uncertainties must be propagated throughout the PDFs, therefore we must further propagate into our final physics analysis. PDFs are updated by the collaborations who perform the fits, such as CTEQ \cite{PhysRevD.78.013004}, each time new data or theoretical predictions become available. The set of PDFs used in this analysis are taken from the CT10 \cite{PhysRevD.89.033009} set. CT10 provides the nominal PDF weight along with 25 free parameters to describe the parton distributions, and thus 25 eigenvalues, providing 50 alternative weights per event. In order to access the weights, we use the Les Houches Accord Parton Distribution Function (LHAPDF) library \cite{Whalley:2005nh}. 

We take the difference between each of the weights and the nominal and add in quadrature, where the final result is then used to calculate the systematic uncertainty associated with the PDFs. %%%%%%%How PDF uncertainties affect the analysis



\subsection{Pile-up Re-weighting} \label{subsec-PUReweightingUncertainties}

Another example of a process that is not described well in simulation compared to data is PU. Additional pileup interactions are included within the simulated samples, however the true number of primary vertices in simulation does not match the number observed in data correctly. This discrepancy between simulation and data gives rise to an incorrect estimation of signal and/or background events in an analysis. In order to correct for this effect, additional corrections must be applied to all simulated samples. The PU re-weighting comes into fruition when dealing with the ever changing instantaneous luminosity of the beams, and thus the change in number of primary vertices in a single data-taking period. In order to implement the PU re-weighting, the number of primary vertices is re-weighted to match the current running conditions in the LHC, for example the number of primary vertices changes with the energy and luminosity of the beams. The obtained uncertainty is then included in the systematic uncertainty on the final results of the analysis. 

We take the number of primary vertices directly from the minimum bias data obtained over the running period in question. We then vary the minimum bias cross-section by $\pm5\%$ which then recalculates the primary vertex distributions which we then use to measure the total impact of the pile-up on the analysis when more, or less, pile-up is observed within the data. %%%%% PU in ttgamma

% Maybe mention out of time pileup?

\subsection{Jet Energy Corrections} \label{subsec-JECUncertainty}

As described in Section \ref{subsec-JEC}, it is necessary to apply corrections to reconstructed jet energies in order to counteract the discrepancies between generator level and detector level jets. These jet energy corrections are a set of tools included to account for non-linearities in the calorimeter, and to give a flat jet response in $\eta$ and E$_T$ as it is not trivial to translate the measured jet energy to the true particle or parton energy. The resulting jet energy corrections and associated uncertainties are measured by the JEC group who then provide the results to the collaboration to be used in analysis \cite{1748-0221-6-11-P11002, CMS-DP-2013-033}.

When we change the jet energy scale (JES) in analysis, the kinematics of each jet within an event are also modified. As a result, the number of jets that pass, or fail, the event selection requirements is likely to change, whereby altering the final topology of an event. This will have a significant impact on the final result. In order to measure the JES significance, the correction factors are varied up and down by one standard deviation, $\sigma$, and propagating the effects through to the MET. %%%Talk about results

The jet energy resolution (JER) is measured as the standard deviation of a Gaussian that is fitted to the jet response of the detector. The JER in data has been found to be worse than the JER in simulation, $\sim 10\%$ broader, and has an associated uncertainty of a similar size \cite{CMS:2011esa}. We correct for this effect by smearing the 4-momentum of jets in MC as a function of the true and reconstructed p$_T$ and $\eta$. To obtain our up and down systematic samples for the jet energy resolution, which are then included within the analysis as nuisance parameters, the smearing is applied twice for up and not at all for down. %%%Impact on JER in ttgamma

% The impact of the JER uncertainty is found to be small in
% both analyses, usually impacting event yields by less than a percent. There are notable
% exceptions to this, for example the tW Z+jets sample, but this is accounted for by the
% limited statistics in the simulated samples. 

\subsection{Missing Transverse Energy} \label{subsec-METUncertainty}

Events that contain neutrinos in the final state are affected mostly by uncertainties from modelling of MET from simulation. The way that the MET is calculated is by taking the sum of the p$_T$ of all PF-reconstructed objects, including `unclustered' energy deposits, and thus uncertainties from these propagate into the calculation of the MET. Unclustered energy is defined as recorded energy deposits that have a low p$_T$ and/or not included in a calorimeter energy deposit cluster due to isolation requirements. PF-reconstructed objects are already corrected for during the reconstruction process ($\rho$-correction, etc), however this is not the case for unclustered energy deposits. Because the unclustered energy is not corrected for during reconstruction, this is where the largest, most prominent source of uncertainty arises. In order to measure the uncertainty on the MET, we remove the p$_T$ of all PF-reconstructed objects from the MET calculation, the residual energy is scaled up and down by 10\%. Other uncertainties that affect the MET, such as JES and JER, are propagated on calculation and thus included in their respective uncertainties. Effects ee and mumu channels due to MET cut and neutrino final states %talk about effect on analysis

% effects of PU on MET distribution corrected in simulation using scale factors obtained from a Z+jets encriched control region.
% The difference between the original and scaled event yields is used as the uncertainty on the background normalisation arising from this reweighting. This uncertainty,
% which only affects the reweighted Z+jets sample in the tW search, is found to be very
% large, especially in the ttbar control regions where there are limited statistics in the simulated
% samples.



\subsection{B-tagging Efficiency} \label{subsec-BTagEfficiency}

% The b-tagging scale factors were applied differently in the
% two analyses, and the uncertainties were also, therefore, treated differently.

Studies of b-tagging efficiencies and misidentification rates are conducted by the b-tag and vertexing group (BTV) and scale factors are produced to correct for discrepancies between data and MC simulation. For the Run I data-taking period at 8 TeV, the BTV group performed studies using $t\bar{t}$ and multi-jet samples \cite{BTAGPAS}. The given samples were chosen such that studies could be performed using events with at least two jets, and a choice of the number of leptons. %b-tagging SFs in ttgamma

% \subsection{Data-driven Re-weighting} \label{subsec-DataDriverReweightingUncertainties}



% Similarly to the uncertainty introduced in Section 7.2.4, the reweighting of the recon-
% structed Z boson p T in the data driven Z+jets background estimation described in Section
% 6.2.2 also introduces a systematic uncertainty. As with the  
% E T modelling, the difference
% between the default and reweighted event yields is used as the uncertainty on the back-
% ground normalisation

\section{Modelling Uncertainties} \label{sec-ModellingUncertainties}

As well as uncertainties that affect the shape and normalisation of simulated distributions, we include uncertainty on the generator production of simulated samples. This arises due to our, possibly, limited understanding of fundamental physical principles of particle interactions in the Standard Model. It is possible that a relatively small shift in paramter value upon generation could produce significantly different events, both kinematically and topologically. We account for this uncertanty by producing alternative datasets where the values of certain parameters are increased and decreased, then by applying the standard event selection we are able to measure the shift in distribution shape from the nominal values. 

\subsection{Renormalisation and factorisation scales}

Upon generation of simulated events, we use Parton Distribution Functions (PDFs) as functions of the factorisation and normalisation scales. In simulated events they are parameterised as as one variable, known as $Q^2$. For a hard-scattering event involving a top quark, we define $Q^2$ to be $Q^2 = m^2_{top} + \sum p^2_T$. In order to calculate the uncertainty due to the renormalisation and factorisation scale, dedicated samples are produced where the $Q^2$ is ``scaled up" and ``scaled down" by a factor of 0.5 and 2.0, respectively. It is to be noted that, as the scaled samples are produced centrally, they also include variations in ISR and FSR which is central to the result in this analysis.

\subsection{Parton-level matching thresholds}

As previously mentioned, simulated events are produced using a hard-scattering generated by using the $\MADGRAPH$ matrix-element (ME) generator. However, the parton showering (PS) and hadronisation of decay products within a generated hard-scattering event is produced using the $\PYTHIA$ event generator. In order to ensure a smooth transition between the two event generators, the parton showering must be matched to the matrix element. The matching process relies on the so called k-factor MLM parton matching scale, whereby the threshold is usually set to 20 GeV \cite{Mangano:2006rw}. In order to calculate the uncertainty of the matching, dedicated samples are produced where the threshold is set to 10 and 40 Gev, respectively, which are then used to calculate the impact varying the threshold has on the final result.

We must note that this systematic only applies to samples that use matrix element to parton shower matching, and therefore does not apply to all samples used in this analysis. The samples that this applies to in this analysis are: $t\bar{t}$  %%%%

\section{Impact of uncertainties}

When a systematic uncertainty is applied, the photon purity, top quark purity, and likelihood fit are recalculated and the new value of the cross-section ratio is measured against the nominal value. Tables \ref{tab-systuncertsMuMu}, \ref{tab-systuncertsEE}, and \ref{tab-systuncertsEMu} list the calculated systematic uncertainties in decreasing order of their impact on the measured cross-section ratio for each decay channel, respectively. 

The statistical uncertainty for the number of signal events, found by maximising the likelihood fit, is the most dominant uncertainty on the cross-section calculation of the $t\bar{t}+\gamma$ process, as is expected. The measurement includes the uncertainties on the measurement of the photon purity, top purity after photon selection, and the statistical uncertainty from the number of events in data. The contribution from each process is measured individually, where the likelihood is calculated such that the uncertainty from each parameter is set to zero each time. In essence, this fixes the value to the measured value, and therefore the change in $SF_{t\bar{t}+\gamma}$ uncertainty (arounf 14\% in the standard likelihood fit) can be attributed to the fixed value. The uncertainty is dominated by the uncertainties obtained from the photon and top purity, contributing 10\% and 9\%, respectively. Therefore, the statistical uncertainty calculated from the number of events in data is approximately 4.8\%. Table \ref{tab-systsamples} shows the simulated $t\bar{t}$ samples used to calculate systematic uncertainties.

\begin{table}[h!] 
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Source} & {\textbf{Ratio Change (\%)} \\
\hline
Statistical likelihood fit & +30.4 / -30.4 \\
Top Quark Mass & +10.0 / -12.0 \\
Fact. and Renorm. Scale & +4.2 / -5.7 \\
ME/PS Threshold & +3.0 / -2.7\\
Jet Energy Scale (JES) & +2.4 / -1.4\\
Photon Energy Scale & +1.8 / -1.7\\
Jet Energy Resolution (JER) & +1.2 / -1.7\\
Top P$_{\text{T}}$ Reweighting & +1.0 / -1.0\\
B-tagging Scale Factor & +1.4 / -0.9\\
Pileup & +1.0 / -0.5\\
Muon Energy Scale & +1.0 / -0.2\\
% PDF & & \\
\hline
Total & +32.7 / -33.4 \\
\hline
\end{tabular} 
\caption{Systematic uncertainties listed in descending order of their contribution to the cross-section ratio in the $\mu^+\mu^-$ channel.}
\label{tab-systuncertsMuMu}
\end{table}

\begin{table}[h!] 
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Source} & {\textbf{Ratio Change (\%)} \\
\hline
Statistical likelihood fit & +37.2 / -37.2 \\
ME/PS Threshold & +14.6 / -6.4\\
Jet Energy Scale (JES) & 7.4 / -4.4\\
Fact. and Renorm. Scale & +6.7 / -4.3\\
Jet Energy Resolution (JER) & +6.2 / -4.4\\
Top Quark Mass &  +4.7 / -5.0\\
B-tagging Scale Factor & +1.2 / -0.8\\
Photon Energy Scale & +1.2 / -2.0\\
Electron Energy Scale Factor & +1.2 / -0.4\\
Pileup & +1.0 / -0.7 \\
Top P$_{\text{T}}$ Reweighting & +0.2 / -0.2\\
% PDF & & \\
\hline
Total & +42.0 / -38.9 \\
\hline
\end{tabular} 
\caption{Systematic uncertainties listed in descending order of their contribution to the cross-section ratio in the $e^+e^-$ channel.}
\label{tab-systuncertsEE}
\end{table}

\begin{table}[h!] 
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Source} & {\textbf{Ratio Change (\%)} \\
\hline
Statistical likelihood fit & +36.4 / -36.4\\
ME/PS Threshold & +10.2 / -5.0 \\
Top Quark Mass &  +7.7 / -7.9\\
Fact. and Renorm. Scale & +6.0 / -5.5\\
Jet Energy Scale (JES) & +5.5 / -6.0\\
Jet Energy Resolution (JER) & +4.2 / -5.0\\
Photon Energy Scale & +1.5 / -2.0\\
B-tagging Scale Factor & +1.4 / -1.0\\
Electron Energy Scale Factor & +1.2 / -0.5\\
Pileup & +1.0 / -0.7\\
Top P$_{\text{T}}$ Reweighting & +1.0 / -0.4\\
Muon Energy Scale Factor & +1.0 / -0.3\\
% PDF & & \\
\hline
Total & +39.8 / -38.9\\
\hline
\end{tabular} 
\caption{Systematic uncertainties listed in descending order of their contribution to the cross-section ratio in the $e\mu$ channel.}
\label{tab-systuncertsEMu}
\end{table}

\begin{sidewaystable}
\begin{center}
% \resizebox{\textwidth}{!} {
\begin{tabular}{|l|l|} %p{11.5cm}
\hline
	Systematic & Dataset \\
\hline
$Q^2$ Scale Up & /TTJets\_MSDecays\_scaleup\_TuneZ2star\_8TeV-madgraph-tauola \\
$Q^2$ Scale Down & /TTJets\_MSDecays\_scaledown\_TuneZ2star\_8TeV-madgraph-tauola \\
\hline
ME/PS Matching Up & /TTJets\_MSDecays\_matchingup\_TuneZ2star\_8TeV-madgraph-tauola \\
ME/PS Matching Down & /TTJets\_MSDecays\_matchingdown\_TuneZ2star\_8TeV-madgraph-tauola \\
\hline
Top Mass Up & /TTJets\_MSDecays\_mass173\_5\_TuneZ2star\_8TeV-madgraph-tauola \\
Top Mass Down & /TTJets\_MSDecays\_mass171\_5\_TuneZ2star\_8TeV-madgraph-tauola \\
\hline
\end{tabular}
\end{center}
\caption{Simulated samples for $t\bar{t}$ systematic uncertainties.}
\label{tab-systsamples}
\end{sidewaystable}